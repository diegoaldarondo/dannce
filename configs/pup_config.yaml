# path to folder where randomly sampled images will be saved
RESULTSDIR: ./

# names of data files. must match the order of CAMNAMES below
datafile: ['CameraLmouse_MatchedFrames', 'CameraRmouse_MatchedFrames', 'CameraSmouse_MatchedFrames']
datadir: ./data/

# Names for each of the camera subdirectories in the video folder. Order should match the datafiles above.
CAMNAMES: ['CameraLmouse', 'CameraRmouse', 'CameraSmouse']
calib_file: ['hires_cam2_params.mat','hires_cam3_params.mat','hires_cam1_params.mat']

# Degree of downsampling applied to image input. Default 1.
DOWNFAC: 2

# Video file extension
extension: '.mp4'

# Path to the COM config file.
COM_CONFIG: ./COM/config.yaml

# Path to the DANNCE config file
DANNCE_CONFIG: ./DANNCE/config.yaml



######################################################################### DANNCE 

_N_VIEWS: 3

# How to crop the input images. The U-Net expects each dimension size to be a multiple of 32.
CROP_HEIGHT: [0, 1024]

CROP_WIDTH: [20, 1300]

# Number of channels for each input image (e.g. RGB == 3)
N_CHANNELS_IN: 3

# If training from scratch, set to the desired number of output channels (i.e. keypoints)
# If fine-tuning, this must match the previous number of output channels, and the new desired
# number is set by NEW_N_CHANNELS_OUT
N_CHANNELS_OUT: 20

# New number of network output channels.
NEW_N_CHANNELS_OUT: 16

# New size of the final output kernel
NEW_LAST_KERNEL_SIZE: [3,3,3]

# BATCH_SIZE
BATCH_SIZE: 8

# Number of parallel workers serving data to the model
WORKERS: 8

# Max. number of batches in multi-processing queue
MAX_QUEUE_SIZE: 10

# DANNCE training option. Sets the size of the 3D Guassians (in mm) used as labels for the MAX models
SIGMA: 10

# DANNCE training option. Sets the number of epochs during training
EPOCHS: 500

# DANNCE training option. Sets the verbosity of training output
VERBOSE: 1

# DANNCE training option. Loss function to be used. Default MSE.
loss: mask_nan_keep_loss

# DANNCE training option. Learning rate for the Adam optimizer. Default 1e-3.
lr: 1e-3

# name of the network architecture (see nets.py)
net: finetune_AVG

# By default, will load in the first hdf5 file at this location for fine-tuning. If training from scratch, set to None
weights: ./weights/ratAVG_3cam/

# whether to use batch normalization or instance normalization. cannot both be true. If fine-tuning, this must be the same setting as used for the initial network
batch_norm: False

instance_norm: True

# Options:
# 'new': initializes and trains a network from scratch
# 'finetune': loads in pre-trained weights and fine-tuned from there
# 'continued': initializes a full model, including optimizer state, and continuous training from the last full model checkpoint
train_mode: finetune

# When fine-tuning, sets the number of layers, starting from the input layer, to lock during training. Default 2
N_LAYERS_LOCKED: 2


# If true, each camera's video directory contains only the video files. If false, each camera's video directory contains an additional subdirectory, which then contains the video files
vid_dir_flag: False

# DANNCE training. Metric to be monitored in addition to loss
metric: ['mse']

# How many samples from each animal do you want to (randomly) set aside for a validation metric?
num_validation_per_exp: 4
 
# When using a system with multiple GPUs, we should just target one ofthem
gpuID: "0"

# The number of frames in each video file
chunks: 3500

# Limits (in mm) to 3D volumes anchored on subject
VMIN: -80
VMAX: 80

# Number of voxels along each spatial dimension
NVOX: 64

# Interpolation mode.
INTERP: linear

retriangulate: False

# If DEPTH is true, will append depth information when sampling images. Particularly useful when using just 1 cameras.
DEPTH: False

IMMODE: 'vid'

# DANNCE training option. Whetehr to turn on rotation augmentation during training
ROTATE: True

# Whether to apply lens distortion during sampling. Default True
DISTORT: True

# If true, intializes an "AVG" version of the network (i.e. final spatial expected value output layer). If false, "MAX" version
EXPVAL: True

# COM finder output confidence scores less than this threshodl will be discarded
comthresh: 0

# If True, will weight the COM estimate in each camera by its confidence score
weighted: False

# Method or combining 3D COMs across camera pairs. Options: 'median', 'mean'
com_method: 'median'

# If the 3D COM has a coordinate beyond this value (in mm), discard it as an error. How large is your arena?
cthresh: 600

# Dictates whether or not to randomly shuffle the camera order when processing volumes. Options: None, 'random'
CHANNEL_COMBO: 'None'

# max. number of batches to evaluate during prediction. set to 'max' to evaluate over all data/video frames
maxbatch: max

# For debugging, True: use allcams for COM, even when using fewer cameras
allcams: False

retriangulate: False

predict_mode: 'torch'