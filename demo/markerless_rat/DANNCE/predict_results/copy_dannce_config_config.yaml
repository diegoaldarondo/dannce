# path to folder where DANNCE weights and logs will be saved
RESULTSDIR: ./DANNCE/train_results/
RESULTSDIR_PREDICT: ./DANNCE/predict_results/

# How to crop the input images. The U-Net expects each dimension size to be a multiple of 32.
CROP_HEIGHT: [0, 1024]
CROP_WIDTH: [20, 1300]

# Number of channels for each input image (e.g. RGB == 3)
N_CHANNELS_IN: 3

# Number of network output channels.
N_CHANNELS_OUT: 20

# BATCH_SIZE
BATCH_SIZE: 4

# Number of parallel workers serving data to the model
WORKERS: 2

# Max. number of batches in multi-processing queue
MAX_QUEUE_SIZE: 10

# DANNCE training option. Sets the size of the 3D Guassians (in mm) used as labels for the MAX models
SIGMA: 10

# DANNCE training option. Sets the number of epochs during training
EPOCHS: 100

# DANNCE training option. Sets the verbosity of training output
VERBOSE: 1

# Degree of downsampling applied to image input. Default 1, other values untested.
DOWNFAC: 1

# DANNCE training option. Loss function to be used. Default MSE.
loss: mask_nan_keep_loss

# DANNCE training option. Learning rate for the Adam optimizer. Default 1e-3.
lr: 5e-5

# During prediction, will look for the last epoch weights saved to ./DANNCe/train_results/. To load in a different weights file, add the path here
# Note that this must be a FULL MODEL file, not just weights.
#predict_model: ./

COM3D_DICT: ./COM/predict_results/COM3D_undistorted.mat

# name of the network architecture (see nets.py)
net: unet3d_big_tiedfirstlayer_expectedvalue
batch_norm: False
instance_norm: True

# If true, each camera's video directory contains only the video files. If false, each camera's video directory contains an additional subdirectory, which then contains the video files
vid_dir_flag: False

# DANNCE training. Metric to be monitored in addition to loss
metric: mse

# How many samples from each animal do you want to (randomly) set aside for a validation metric?
num_validation_per_exp: 5

# If true, saves plots of the training labels overlaid on images
debug: True

# Path to load in a specific set of validation sampleIDs
#load_valid: 

# Path to a list of experiment config files. By Default, train_COMfinder.py will look for exp*.yaml files in the same directory as this file
#exp_path: [exp1.yaml, exp2.yaml, exp3.yaml]

# When using a system with multiple GPUs, we should just target one ofthem
gpuID: "0"

# How many frames to you want to predict over? Set to 'max' for all video frames.
max_num_samples: 1000

# The number of frames in each video file
chunks: 3500

# path to COM file. By Default, during prediction DANNCE will look in ./COM/predict_results/ for a COM_undistorted.mat file
#COMfilename: ./

# Limits (in mm) to 3D volumes anchored on subject
VMIN: -120
VMAX: 120
# Number of voxels along each spatial dimension
NVOX: 64

# Interpolation mode.
INTERP: nearest

# If DEPTH is true, will append depth information when sampling images. Particularly useful when using just 1 cameras.
DEPTH: False

IMMODE: 'vid'

# DANNCE training option. Whetehr to turn on rotation augmentation during training
ROTATE: False

# Whether to apply lens distortion during sampling. Default True
DISTORT: True

# If true, intializes an "AVG" version of the network (i.e. final spatial expected value output layer). If false, "MAX" version
EXPVAL: True

# COM finder output confidence scores less than this threshodl will be discarded
comthresh: 0

# If True, will weight the COM estimate in each camera by its confidence score
weighted: False

# Method or combining 3D COMs across camera pairs. Options: 'median', 'mean'
com_method: 'median'

# If the 3D COM has a coordinate beyond this value (in mm), discard it as an error. How large is your arena?
cthresh: 300

# Dictates whether or not to randomly shuffle the camera order when processing volumes. Options: None, 'random'
CHANNEL_COMBO: None

# max. number of batches to evaluate. set to 'max' to evaluate over all data/video frames
maxbatch: max

retriangulate: False